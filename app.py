import streamlit as st
import docx
from docx.shared import Pt, Inches
from docx.oxml.ns import qn
import random
import io
import pandas as pd
import time
import base64
import requests 
from PIL import Image
try:
    from streamlit_cropper import st_cropper 
except ImportError:
    st_cropper = None 
except Exception:
    st_cropper = None

import os
import datetime
import uuid
import json
from google.cloud import firestore
from google.cloud import storage
import google.auth 
from google.oauth2 import service_account

import smart_importer

st.set_page_config(page_title="ç‰©ç†é¡Œåº«ç³»çµ± (Pro)", layout="wide", page_icon="ğŸ§²")

# é¡Œå‹å°ç…§è¡¨
TYPE_MAP_ZH_TO_EN = {"å–®é¸": "Single", "å¤šé¸": "Multi", "å¡«å……": "Fill", "é¡Œçµ„": "Group"}
TYPE_MAP_EN_TO_ZH = {v: k for k, v in TYPE_MAP_ZH_TO_EN.items()}
TYPE_OPTIONS = ["å–®é¸", "å¤šé¸", "å¡«å……", "é¡Œçµ„"]

# ==========================================
# é›²ç«¯è³‡æ–™åº«èˆ‡å„²å­˜æ¨¡çµ„ (å…§å»º)
# ==========================================
class CloudManager:
    def __init__(self):
        self.bucket_name = os.getenv("GCS_BUCKET_NAME", "physics-exam-assets")
        self.db = None
        self.storage_client = None
        self.has_connection = False
        self.connection_error = ""
        self.project_id = None
        self.credentials = None 

        try:
            # ç­–ç•¥ 1ï¼šç’°å¢ƒè®Šæ•¸ JSON (Cloud Run å„ªå…ˆ)
            service_account_json = os.getenv("GCP_SERVICE_ACCOUNT_JSON")
            if service_account_json:
                try:
                    service_account_json = service_account_json.strip()
                    if service_account_json.startswith("'") and service_account_json.endswith("'"):
                         service_account_json = service_account_json[1:-1]
                    
                    service_account_info = json.loads(service_account_json)
                    self.credentials = service_account.Credentials.from_service_account_info(service_account_info)
                    self.project_id = service_account_info.get("project_id")
                    
                    if not self.project_id:
                         self.project_id = os.getenv("GCP_PROJECT_ID")

                    self.db = firestore.Client(credentials=self.credentials, project=self.project_id)
                    self.storage_client = storage.Client(credentials=self.credentials, project=self.project_id)
                    self.has_connection = True
                    if self.has_connection: self._ensure_bucket_exists()
                    return
                except Exception as e:
                    print(f"ç’°å¢ƒè®Šæ•¸ JSON é€£ç·šå¤±æ•—: {e}")

            # ç­–ç•¥ 2ï¼šStreamlit Secrets (Streamlit Cloud)
            try:
                if "gcp_service_account" in st.secrets:
                    try:
                        service_account_info = st.secrets["gcp_service_account"]
                        self.credentials = service_account.Credentials.from_service_account_info(service_account_info)
                        
                        self.project_id = service_account_info.get("project_id")
                        self.db = firestore.Client(credentials=self.credentials, project=self.project_id)
                        self.storage_client = storage.Client(credentials=self.credentials, project=self.project_id)
                        self.has_connection = True
                        if self.has_connection: self._ensure_bucket_exists()
                        return 
                    except Exception as e:
                        print(f"Secrets é€£ç·šå¤±æ•—: {e}")
            except: pass

            # ç­–ç•¥ 3ï¼šè‡ªå‹•åµæ¸¬
            self.project_id = (os.getenv("GCP_PROJECT_ID") or os.getenv("GOOGLE_CLOUD_PROJECT"))
            
            if not self.project_id:
                try:
                    self.credentials, project_id_from_auth = google.auth.default()
                    if project_id_from_auth:
                        self.project_id = project_id_from_auth
                except: pass

            if self.project_id:
                if self.credentials:
                    self.db = firestore.Client(credentials=self.credentials, project=self.project_id)
                    self.storage_client = storage.Client(credentials=self.credentials, project=self.project_id)
                else:
                    self.db = firestore.Client(project=self.project_id)
                    self.storage_client = storage.Client(project=self.project_id)
                self.has_connection = True
            else:
                try:
                    self.db = firestore.Client()
                    self.storage_client = storage.Client()
                    self.has_connection = True
                except: pass
            
            if self.has_connection: self._ensure_bucket_exists()

        except Exception as e:
            self.connection_error = str(e)
            print(f"Cloud é€£ç·šåˆå§‹åŒ–å¤±æ•—: {e}")

    def _ensure_bucket_exists(self):
        if not self.storage_client: return
        try:
            target_bucket_name = self.bucket_name
            if not target_bucket_name:
                try:
                    if "GCS_BUCKET_NAME" in st.secrets:
                        target_bucket_name = st.secrets["GCS_BUCKET_NAME"]
                except: pass
            
            if target_bucket_name:
                bucket = self.storage_client.bucket(target_bucket_name)
                if not bucket.exists():
                    bucket.create(location="us-central1") 
        except: pass

    # --- å®¹é‡è¨ˆç®— ---
    def get_storage_usage(self):
        if not self.storage_client: return 0
        try:
            target_bucket_name = self.bucket_name
            if not target_bucket_name:
                try:
                    if "GCS_BUCKET_NAME" in st.secrets:
                        target_bucket_name = st.secrets["GCS_BUCKET_NAME"]
                except: pass
            
            if not target_bucket_name: return 0

            bucket = self.storage_client.bucket(target_bucket_name)
            blobs = bucket.list_blobs()
            total_bytes = sum(blob.size for blob in blobs if blob.size is not None)
            return total_bytes
        except Exception as e:
            print(f"å®¹é‡è¨ˆç®—å¤±æ•—: {e}")
            return 0

    # --- æ ¸å¿ƒä¿®å¾©ï¼šä¸Šå‚³èˆ‡ä¸‹è¼‰ ---
    def upload_bytes(self, file_bytes, filename, folder="uploads", content_type=None):
        """ä¸Šå‚³æª”æ¡ˆï¼Œå›å‚³ (å…¬é–‹ç¶²å€, Blobåç¨±)"""
        if not self.storage_client: return None, None
        try:
            target_bucket_name = self.bucket_name
            if not target_bucket_name:
                try:
                    if "GCS_BUCKET_NAME" in st.secrets:
                        target_bucket_name = st.secrets["GCS_BUCKET_NAME"]
                except: pass
            
            if not target_bucket_name:
                st.error("æœªè¨­å®š Bucket åç¨±")
                return None, None

            bucket = self.storage_client.bucket(target_bucket_name)
            unique_name = f"{folder}/{int(datetime.datetime.now().timestamp())}_{str(uuid.uuid4())[:8]}_{filename}"
            blob = bucket.blob(unique_name)
            blob.upload_from_string(file_bytes, content_type=content_type)
            
            url = blob.public_url
            try:
                # å˜—è©¦ç”¢ç”Ÿ Signed URL
                if self.credentials and hasattr(self.credentials, 'service_account_email'):
                     url = blob.generate_signed_url(
                        version="v4",
                        expiration=datetime.timedelta(days=7),
                        method="GET",
                        service_account_email=self.credentials.service_account_email,
                        access_token=self.credentials.token
                    )
                else:
                    url = blob.generate_signed_url(
                        version="v4",
                        expiration=datetime.timedelta(days=7),
                        method="GET"
                    )
            except: pass
            
            return url, unique_name # å›å‚³ Tuple

        except Exception as e:
            print(f"ä¸Šå‚³å¤±æ•—: {e}")
            return None, None

    def download_blob(self, blob_name):
        """ç›´æ¥é€é API ä¸‹è¼‰ Blobï¼Œä¸éœ€ç¶“é URL (è§£æ±ºä¸‹è¼‰ç•°å¸¸æœ€æœ‰æ•ˆçš„æ–¹æ³•)"""
        if not self.storage_client or not blob_name: return None
        try:
            target_bucket_name = self.bucket_name
            if not target_bucket_name:
                try:
                    if "GCS_BUCKET_NAME" in st.secrets:
                        target_bucket_name = st.secrets["GCS_BUCKET_NAME"]
                except: pass
                
            bucket = self.storage_client.bucket(target_bucket_name)
            blob = bucket.blob(blob_name)
            return blob.download_as_bytes()
        except Exception as e:
            print(f"Blob ä¸‹è¼‰å¤±æ•—: {e}")
            return None

    # --- æª”æ¡ˆåº«ç®¡ç† ---
    def check_file_exists(self, filename):
        if not self.db: return None
        try:
            docs = self.db.collection("exam_files").where("filename", "==", filename).limit(1).stream()
            for doc in docs:
                data = doc.to_dict()
                data['id'] = doc.id
                return data 
            return None
        except Exception as e:
            print(f"æª¢æŸ¥æª”æ¡ˆå¤±æ•—: {e}")
            return None

    def save_file_record(self, file_info, overwrite_id=None):
        if not self.db: return False
        try:
            doc_id = overwrite_id if overwrite_id else str(uuid.uuid4())
            file_info["id"] = doc_id
            file_info["updated_at"] = datetime.datetime.now()
            self.db.collection("exam_files").document(doc_id).set(file_info)
            return True
        except Exception as e:
            st.error(f"å„²å­˜æª”æ¡ˆè¨˜éŒ„å¤±æ•—: {e}")
            return False

    def load_file_records(self):
        if not self.db: return []
        try:
            files = []
            docs = self.db.collection("exam_files").order_by("updated_at", direction=firestore.Query.DESCENDING).stream()
            for doc in docs:
                files.append(doc.to_dict())
            return files
        except Exception as e:
            st.error(f"è®€å–æª”æ¡ˆåˆ—è¡¨å¤±æ•—: {e}")
            return []

    def delete_file_record(self, file_id):
        if self.db:
            self.db.collection("exam_files").document(file_id).delete()

    def update_file_status(self, file_id, status):
        if self.db:
            self.db.collection("exam_files").document(file_id).update({"ai_status": status})

    # --- é¡Œåº«ç®¡ç† ---
    def save_question(self, question_dict):
        if not self.db: return False
        try:
            if question_dict.get("image_data_b64"):
                try:
                    img_bytes = base64.b64decode(question_dict["image_data_b64"])
                    fname = f"q_{question_dict.get('id', 'unknown')}.png"
                    img_url, _ = self.upload_bytes(img_bytes, fname, folder="question_images", content_type="image/png")
                    if img_url:
                        question_dict["image_url"] = img_url
                        del question_dict["image_data_b64"]
                except Exception as e:
                    print(f"åœ–ç‰‡è½‰å­˜å¤±æ•—: {e}")
            
            self.db.collection("questions").document(question_dict["id"]).set(question_dict)
            return True
        except Exception as e:
            st.error(f"å„²å­˜é¡Œç›®å¤±æ•—: {e}")
            return False

    def load_questions(self):
        if not self.db: return []
        try:
            questions = []
            docs = self.db.collection("questions").order_by("id").stream()
            for doc in docs:
                questions.append(doc.to_dict())
            return questions
        except Exception as e:
            st.error(f"è®€å–é¡Œåº«å¤±æ•—: {e}")
            return []

    def delete_question(self, doc_id):
        if self.db:
            self.db.collection("questions").document(doc_id).delete()

    # --- æš«å­˜æ‰¹æ¬¡ç®¡ç† ---
    def save_temp_batch(self, file_id, batch_idx, data, status="success"):
        if not self.db: return
        serializable_data = []
        for cand in data:
            if isinstance(cand, dict): d = cand
            else: d = cand.__dict__.copy()
            d.pop('image_bytes', None)
            d.pop('ref_image_bytes', None) 
            d.pop('full_page_bytes', None)
            serializable_data.append(d)

        doc_ref = self.db.collection("temp_batches").document(f"{file_id}_{batch_idx}")
        doc_ref.set({
            "file_id": file_id,
            "batch_idx": batch_idx,
            "data": json.dumps(serializable_data), 
            "status": status,
            "updated_at": datetime.datetime.now()
        })

    def load_temp_batches(self, file_id):
        if not self.db: return {}
        try:
            docs = self.db.collection("temp_batches").where("file_id", "==", file_id).stream()
            batches = {}
            for doc in docs:
                d = doc.to_dict()
                batches[d['batch_idx']] = d
            return batches
        except Exception as e:
            print(f"è¼‰å…¥æš«å­˜å¤±æ•—: {e}")
            return {}

    def clear_temp_batches(self, file_id):
        if not self.db: return
        try:
            docs = self.db.collection("temp_batches").where("file_id", "==", file_id).stream()
            for doc in docs: doc.reference.delete()
        except: pass

# åˆå§‹åŒ– Cloud Manager
cloud_manager = CloudManager()

# ==========================================
# è³‡æ–™çµæ§‹èˆ‡ç‹€æ…‹åˆå§‹åŒ–
# ==========================================
class Question:
    def __init__(self, q_type, content, options=None, answer=None, original_id=0, image_data=None, 
                 source="ä¸€èˆ¬è©¦é¡Œ", chapter="æœªåˆ†é¡", unit="", db_id=None, 
                 parent_id=None, is_group_parent=False, sub_questions=None, image_url=None,
                 source_file_id=None):
        self.id = db_id if db_id else str(int(time.time()*1000)) + str(random.randint(0, 999))
        self.type = q_type 
        self.source = source
        self.chapter = chapter
        self.unit = unit
        self.content = content
        self.options = options if options else []
        self.answer = answer
        self.image_data = image_data 
        self.image_url = image_url   
        
        self.parent_id = parent_id 
        self.is_group_parent = is_group_parent 
        self.sub_questions = sub_questions if sub_questions else [] 
        self.source_file_id = source_file_id

    def to_dict(self):
        img_str = None
        if self.image_data:
            img_str = base64.b64encode(self.image_data).decode('utf-8')
        subs = [q.to_dict() for q in self.sub_questions] if self.sub_questions else []
        return {
            "id": self.id, "type": self.type, "source": self.source, "chapter": self.chapter,
            "content": self.content, "options": self.options, "answer": self.answer,
            "image_data_b64": img_str, "image_url": self.image_url,
            "parent_id": self.parent_id, "is_group_parent": self.is_group_parent,
            "sub_questions": subs, "source_file_id": self.source_file_id
        }

    @staticmethod
    def from_dict(data):
        img_bytes = None
        img_url = data.get("image_url")
        if data.get("image_data_b64"):
            try: img_bytes = base64.b64decode(data["image_data_b64"])
            except: pass
        q = Question(
            q_type=data.get("type", "Single"), content=data.get("content", ""),
            options=data.get("options", []), answer=data.get("answer", ""),
            original_id=0, image_data=img_bytes, image_url=img_url,
            source=data.get("source", ""), chapter=data.get("chapter", "æœªåˆ†é¡"),
            db_id=data.get("id"), parent_id=data.get("parent_id"),
            is_group_parent=data.get("is_group_parent", False),
            source_file_id=data.get("source_file_id")
        )
        if data.get("sub_questions"):
            q.sub_questions = [Question.from_dict(sub) for sub in data["sub_questions"]]
        return q

if 'question_pool' not in st.session_state:
    st.session_state['question_pool'] = []
    try:
        cloud_data = cloud_manager.load_questions()
        if cloud_data:
            st.session_state['question_pool'] = [Question.from_dict(d) for d in cloud_data]
    except: pass

if 'file_queue' not in st.session_state:
    st.session_state['file_queue'] = {}

# ==========================================
# å·¥å…·å‡½å¼
# ==========================================
def get_image_bytes(q):
    if q.image_data: return q.image_data
    if q.image_url:
        try:
            response = requests.get(q.image_url, timeout=3)
            if response.status_code == 200: return response.content
        except: return None
    return None

def generate_word_files(selected_questions):
    exam_doc = docx.Document()
    ans_doc = docx.Document()
    style = exam_doc.styles['Normal']
    style.font.name = 'Times New Roman'
    style.font.size = Pt(12)
    style.element.rPr.rFonts.set(qn('w:eastAsia'), 'æ¨™æ¥·é«”')
    
    exam_doc.add_heading('ç‰©ç†ç§‘ è©¦é¡Œå·', 0)
    ans_doc.add_heading('ç‰©ç†ç§‘ ç­”æ¡ˆå·', 0)
    
    q_counter = 1
    
    def write_single_question(doc, q, idx_str):
        p = doc.add_paragraph()
        type_badge_zh = TYPE_MAP_EN_TO_ZH.get(q.type, q.type)
        type_label = f"ã€{type_badge_zh}ã€‘"
        src_label = f"[{q.source}] " if q.source and not q.parent_id else "" 
        
        runner = p.add_run(f"{idx_str}. {src_label}{type_label} {q.content.strip()}")
        runner.bold = True
        
        img_bytes = get_image_bytes(q)
        if img_bytes:
            try:
                img_p = doc.add_paragraph()
                run = img_p.add_run()
                run.add_picture(io.BytesIO(img_bytes), width=Inches(2.5))
            except: pass

        if q.type in ['Single', 'Multi'] and q.options:
            opts = q.options
            max_len = max([len(str(o)) for o in opts]) if opts else 0
            if max_len < 10 and len(opts) > 0:
                doc.add_paragraph("ã€€ã€€".join(opts))
            elif max_len < 25 and len(opts) > 0 and len(opts) % 2 == 0:
                table = doc.add_table(rows=(len(opts) // 2), cols=2)
                table.autofit = True
                for i, opt in enumerate(opts):
                    table.cell(i // 2, i % 2).text = opt
                doc.add_paragraph("")
            else:
                for opt in opts:
                    doc.add_paragraph(f"{opt}")
        elif q.type == 'Fill':
            doc.add_paragraph("ç­”ï¼š______________________")
        doc.add_paragraph("") 

    for q in selected_questions:
        if q.is_group_parent:
            write_single_question(exam_doc, q, f"{q_counter}-{q_counter + len(q.sub_questions) - 1} ç‚ºé¡Œçµ„")
            for sub_q in q.sub_questions:
                write_single_question(exam_doc, sub_q, str(q_counter))
                ans_p = ans_doc.add_paragraph()
                ans_p.add_run(f"{q_counter}. {sub_q.answer}")
                q_counter += 1
        else:
            write_single_question(exam_doc, q, str(q_counter))
            ans_p = ans_doc.add_paragraph()
            ans_p.add_run(f"{q_counter}. {q.answer}")
            q_counter += 1
        
    exam_io = io.BytesIO()
    ans_io = io.BytesIO()
    exam_doc.save(exam_io)
    ans_doc.save(ans_io)
    exam_io.seek(0)
    ans_io.seek(0)
    return exam_io, ans_io

# æ ¸å¿ƒï¼šåˆ†é æ‰¹æ¬¡è™•ç†é‚è¼¯
def process_file_in_batches(filename, api_key, file_id, batch_size=5, target_batch_idx=None):
    file_bytes = None
    if filename in st.session_state.get('file_queue', {}):
        file_bytes = st.session_state['file_queue'][filename]['data']
    else:
        record = cloud_manager.check_file_exists(filename)
        if record and record.get('blob_name'):
             file_bytes = cloud_manager.download_blob(record['blob_name'])
        elif record and record.get('url'):
            try:
                resp = requests.get(record.get('url'))
                if resp.status_code == 200: file_bytes = resp.content
            except: pass
    
    if not file_bytes:
        st.error("ç„¡æ³•è®€å–æª”æ¡ˆå…§å®¹")
        return

    try:
        from pdf2image import convert_from_bytes
        # é€™è£¡éœ€è¦ poppler-utils
        from pdf2image.pdf2image import pdfinfo_from_bytes
        info = pdfinfo_from_bytes(file_bytes)
        total_pages = info["Pages"]
    except:
        # Fallback: è½‰ç¬¬ä¸€é è©¦è©¦
        try:
            info = convert_from_bytes(file_bytes, size=1) 
            total_pages = len(info)
            if total_pages == 0: total_pages = 20
        except:
             total_pages = 20
    
    num_batches = (total_pages + batch_size - 1) // batch_size
    batches_to_run = range(num_batches) if target_batch_idx is None else [target_batch_idx]

    progress_bar = st.progress(0)
    
    for i, b_idx in enumerate(batches_to_run):
        start_page = b_idx * batch_size
        end_page = min((b_idx + 1) * batch_size, total_pages)
        
        status_text = f"æ­£åœ¨åˆ†æç¬¬ {start_page+1}~{end_page} é ..."
        st.caption(status_text)
        
        res_candidates = smart_importer.parse_with_gemini(
            file_bytes, 'pdf', api_key, target_pages=(start_page, end_page)
        )
        
        if isinstance(res_candidates, list):
            serializable_data = []
            for cand in res_candidates:
                d = cand.__dict__.copy()
                d.pop('image_bytes', None)
                d.pop('ref_image_bytes', None) 
                d.pop('full_page_bytes', None)
                serializable_data.append(d)
            cloud_manager.save_temp_batch(file_id, b_idx, serializable_data, "success")
        else:
            cloud_manager.save_temp_batch(file_id, b_idx, [], "failed")
            st.error(f"ç¬¬ {b_idx+1} æ‰¹æ¬¡å¤±æ•—")

        progress_bar.progress((i + 1) / len(batches_to_run))
        
    cloud_manager.update_file_status(file_id, "å·²è¾¨è­˜")
    st.success("è™•ç†å®Œæˆï¼")
    time.sleep(1)
    st.rerun()

# ==========================================
# Interface
# ==========================================
st.title("ğŸ§² ç‰©ç†é¡Œåº«ç³»çµ± Pro (Cloud Storage)")

with st.sidebar:
    st.header("è¨­å®š")
    env_api_key = os.getenv("GOOGLE_API_KEY", "")
    api_key_input = st.text_input("Gemini API Key", value=env_api_key, type="password", key="sidebar_api_key")
    
    if cloud_manager.has_connection:
        st.success("â˜ï¸ Cloud: å·²é€£ç·š")
        if cloud_manager.bucket_name:
            st.caption(f"Bucket: {cloud_manager.bucket_name}")
    else:
        st.warning(f"â˜ï¸ Cloud: æœªé€£ç·š")
        if cloud_manager.connection_error:
            st.caption(f"éŒ¯èª¤: {cloud_manager.connection_error}")
            if "No secrets found" in cloud_manager.connection_error:
                st.info("Secrets æœªè¨­å®šï¼Œè«‹æ”¹ç”¨ç’°å¢ƒè®Šæ•¸ GCP_SERVICE_ACCOUNT_JSON")

    st.divider()
    st.metric("é¡Œåº«ç¸½æ•¸", len(st.session_state['question_pool']))
    
    if cloud_manager.has_connection:
        st.divider()
        try:
            total_bytes = cloud_manager.get_storage_usage()
            total_mb = total_bytes / (1024 * 1024)
            limit_mb = 1024.0 # 1GB
            percentage = min(total_mb / limit_mb, 1.0)
            
            st.write("ğŸ“Š **é›²ç«¯å„²å­˜ç©ºé–“**")
            st.progress(percentage)
            st.caption(f"å·²ä½¿ç”¨: {total_mb:.2f} MB / 1 GB")
            if percentage > 0.9: st.warning("âš ï¸ å®¹é‡å³å°‡é¡æ»¿ï¼")
        except: st.caption("ç„¡æ³•å–å¾—å®¹é‡è³‡è¨Š")

    if st.button("å¼·åˆ¶å„²å­˜è‡³é›²ç«¯", key="sidebar_force_save"):
        if cloud_manager.has_connection:
            progress_bar = st.progress(0)
            total = len(st.session_state['question_pool'])
            for i, q in enumerate(st.session_state['question_pool']):
                cloud_manager.save_question(q.to_dict())
                progress_bar.progress((i + 1) / total)
            st.success("å„²å­˜å®Œæˆï¼")

# Tabs
tab_upload_process, tab_files, tab_review, tab_bank = st.tabs(["ğŸ§  è€ƒå¤é¡Œä¸Šå‚³", "ğŸ“‚ æª”æ¡ˆç®¡ç†åŠAIè¾¨è­˜", "ğŸ“ AIåŒ¯å…¥æ ¡å°", "ğŸ“š é¡Œåº«ç®¡ç†èˆ‡è©¦å·è¼¸å‡º"])

# === Tab 1: è€ƒå¤é¡Œä¸Šå‚³ ===
with tab_upload_process:
    st.markdown("### ğŸ“¤ ä¸Šå‚³æ–°è€ƒå¤é¡Œ")
    uploaded_files = st.file_uploader("æ”¯æ´ .pdf, .docx", type=['pdf', 'docx'], accept_multiple_files=True)
    
    if uploaded_files:
        st.divider()
        st.subheader("è¨­å®šæª”æ¡ˆè³‡è¨Š")
        if 'upload_configs' not in st.session_state: st.session_state['upload_configs'] = {}

        with st.expander("æ‰¹æ¬¡è¨­å®š (ä¸€æ¬¡å¥—ç”¨çµ¦ä¸‹æ–¹æ‰€æœ‰æª”æ¡ˆ)"):
            c_batch1, c_batch2, c_batch3, c_batch4 = st.columns(4)
            with c_batch1: b_type = st.selectbox("çµ±ä¸€é¡å‹", ["å­¸æ¸¬", "åˆ†ç§‘", "åŒ—æ¨¡", "ä¸­æ¨¡", "å…¨æ¨¡", "å…¶ä»–"], key="batch_type")
            with c_batch2: b_year = st.text_input("çµ±ä¸€å¹´åº¦", value="112", key="batch_year")
            with c_batch3: b_exam_no = st.selectbox("çµ±ä¸€è€ƒè©¦æ¬¡åˆ¥", ["ç¬¬ä¸€æ¬¡", "ç¬¬äºŒæ¬¡", "ç¬¬ä¸‰æ¬¡", "æ­£å¼è€ƒè©¦"], key="batch_no")
            with c_batch4: 
                if st.button("å…¨éƒ¨å¥—ç”¨"):
                    for uf in uploaded_files:
                        st.session_state['upload_configs'][uf.name] = {"type": b_type, "year": b_year, "exam_no": b_exam_no}
                    st.success("å·²å¥—ç”¨ï¼")

        files_to_upload = []
        for i, f in enumerate(uploaded_files):
            current_config = st.session_state['upload_configs'].get(f.name, {"type": "å­¸æ¸¬", "year": "112", "exam_no": "æ­£å¼è€ƒè©¦"})
            with st.container():
                c1, c2, c3, c4 = st.columns([3, 2, 2, 2])
                with c1: 
                    st.markdown(f"**{i+1}. {f.name}**")
                    ext = f.name.split('.')[-1]
                    new_name = f"{current_config['year']}-{current_config['type']}-{current_config['exam_no']}.{ext}"
                    st.caption(f"â `{new_name}`")
                with c2: 
                    new_type = st.selectbox("é¡å‹", ["å­¸æ¸¬", "åˆ†ç§‘", "åŒ—æ¨¡", "ä¸­æ¨¡", "å…¨æ¨¡", "å…¶ä»–"], index=["å­¸æ¸¬", "åˆ†ç§‘", "åŒ—æ¨¡", "ä¸­æ¨¡", "å…¨æ¨¡", "å…¶ä»–"].index(current_config['type']), key=f"type_{f.name}")
                with c3: 
                    new_year = st.text_input("å¹´åº¦", value=current_config['year'], key=f"year_{f.name}")
                with c4: 
                    new_no = st.selectbox("æ¬¡åˆ¥", ["ç¬¬ä¸€æ¬¡", "ç¬¬äºŒæ¬¡", "ç¬¬ä¸‰æ¬¡", "æ­£å¼è€ƒè©¦"], index=["ç¬¬ä¸€æ¬¡", "ç¬¬äºŒæ¬¡", "ç¬¬ä¸‰æ¬¡", "æ­£å¼è€ƒè©¦"].index(current_config['exam_no']), key=f"no_{f.name}")
                st.session_state['upload_configs'][f.name] = {"type": new_type, "year": new_year, "exam_no": new_no}
                final_new_name = f"{new_year}-{new_type}-{new_no}.{f.name.split('.')[-1]}"
                files_to_upload.append({"file_obj": f, "new_filename": final_new_name, "type": new_type, "year": new_year, "exam_no": new_no})
            st.divider()

        if st.button("ç¢ºèªä¸¦ä¸Šå‚³æ‰€æœ‰æª”æ¡ˆ", type="primary"):
            duplicate_warnings = []
            for item in files_to_upload:
                existing = cloud_manager.check_file_exists(item['new_filename'])
                if existing: duplicate_warnings.append(f"{item['new_filename']} (åŸ: {item['file_obj'].name})")
            
            if duplicate_warnings:
                st.error(f"ç™¼ç¾é›²ç«¯å·²æœ‰é‡è¤‡æª”åï¼Œè«‹ä¿®æ”¹å¹´åº¦æˆ–æ¬¡åˆ¥ï¼š\n" + "\n".join(duplicate_warnings))
            else:
                progress_bar = st.progress(0)
                success_count = 0
                for idx, item in enumerate(files_to_upload):
                    f = item['file_obj']
                    new_fname = item['new_filename']
                    f.seek(0)
                    file_bytes = f.read()
                    backup_url, blob_name = cloud_manager.upload_bytes(file_bytes, new_fname, folder="raw_uploads", content_type=f.type)
                    
                    file_record = {
                        "filename": new_fname, "original_filename": f.name, "url": backup_url, "blob_name": blob_name,
                        "exam_type": item['type'], "year": item['year'], "exam_no": item['exam_no'],
                        "ai_status": "æœªè¾¨è­˜", "created_at": datetime.datetime.now()
                    }
                    cloud_manager.save_file_record(file_record)
                    st.session_state['file_queue'][new_fname] = {
                        "status": "uploaded", "data": file_bytes, "type": f.type.split('/')[-1] if '/' in f.type else 'pdf',
                        "result": [], "error_msg": "", "source_tag": f"{item['type']}-{item['year']}",
                        "backup_url": backup_url, "blob_name": blob_name, "db_id": file_record['id']
                    }
                    success_count += 1
                    progress_bar.progress((idx + 1) / len(files_to_upload))
                
                if success_count > 0:
                    st.success(f"æˆåŠŸä¸Šå‚³ {success_count} å€‹æª”æ¡ˆï¼")
                    st.session_state['upload_configs'] = {}
                    time.sleep(1)
                    st.rerun()

    if st.session_state['file_queue']:
        with st.expander(f"æŸ¥çœ‹ç›®å‰å·¥ä½œéšæ®µæš«å­˜ ({len(st.session_state['file_queue'])})"):
            for fname in st.session_state['file_queue']: st.write(fname)

# === Tab 2: æª”æ¡ˆç®¡ç†åŠAIè¾¨è­˜ ===
with tab_files:
    if 'just_processed_file' in st.session_state:
        st.success(f"ğŸ‰ **{st.session_state['just_processed_file']}** è¾¨è­˜å®Œæˆï¼")
        st.info("ğŸ‘‰ è«‹é»é¸ä¸Šæ–¹ **ã€ŒğŸ“ AIåŒ¯å…¥æ ¡å°ã€** åˆ†é é€²è¡Œæª¢æŸ¥ã€‚")
        del st.session_state['just_processed_file']

    st.subheader("å·²ä¸Šå‚³è€ƒå¤é¡Œæª”æ¡ˆåº«")
    cloud_files = cloud_manager.load_file_records()
    
    if not cloud_files:
        st.info("ç›®å‰æ²’æœ‰å·²ä¸Šå‚³çš„æª”æ¡ˆè¨˜éŒ„ã€‚")
    else:
        files_tree = {}
        for f in cloud_files:
            ftype = f.get('exam_type', 'æœªåˆ†é¡')
            fyear = f.get('year', 'æœªçŸ¥å¹´ä»½')
            if ftype not in files_tree: files_tree[ftype] = {}
            if fyear not in files_tree[ftype]: files_tree[ftype][fyear] = []
            files_tree[ftype][fyear].append(f)

        for ftype in sorted(files_tree.keys()):
            with st.expander(f"ğŸ“ {ftype}", expanded=False):
                years_dict = files_tree[ftype]
                def year_sort_key(y_str): return -int(y_str) if y_str.isdigit() else 0
                for fyear in sorted(years_dict.keys(), key=year_sort_key):
                    with st.expander(f"ğŸ“ {fyear} å¹´åº¦", expanded=False):
                        files_list = years_dict[fyear]
                        exam_no_order = {"ç¬¬ä¸€æ¬¡": 1, "ç¬¬äºŒæ¬¡": 2, "ç¬¬ä¸‰æ¬¡": 3, "æ­£å¼è€ƒè©¦": 4, "å…¶ä»–": 99}
                        def file_sort_key(f): return exam_no_order.get(f.get('exam_no', 'å…¶ä»–'), 100)
                        sorted_files = sorted(files_list, key=file_sort_key)
                        
                        for f_record in sorted_files:
                            c_name, c_status, c_action = st.columns([5, 2, 3], vertical_alignment="center")
                            with c_name: st.write(f"ğŸ“„ {f_record.get('filename')}")
                            with c_status:
                                status = f_record.get('ai_status', 'æœªè¾¨è­˜')
                                if status == 'å·²è¾¨è­˜': st.button("âœ… å·²è¾¨è­˜", key=f"status_{f_record['id']}", disabled=True, use_container_width=True)
                                else: st.button("â¬œ æœªè¾¨è­˜", key=f"status_{f_record['id']}", disabled=True, use_container_width=True)
                            with c_action:
                                b1, b2 = st.columns(2)
                                with b1:
                                    btn_label = "é‡æ–°è¾¨è­˜" if status == 'å·²è¾¨è­˜' else "AI è¾¨è­˜"
                                    if st.button(btn_label, key=f"ai_{f_record['id']}", use_container_width=True):
                                        process_file_in_batches(f_record['filename'], api_key_input, f_record['id'])
                                with b2:
                                    if st.button("ğŸ—‘ï¸", key=f"del_f_{f_record['id']}", type="primary", use_container_width=True):
                                        cloud_manager.delete_file_record(f_record['id'])
                                        st.rerun()
                            # æ‰¹æ¬¡ç‹€æ…‹
                            batches = cloud_manager.load_temp_batches(f_record['id'])
                            if batches:
                                with st.expander("æŸ¥çœ‹æ‰¹æ¬¡è™•ç†è©³æƒ… (å¯å–®ç¨é‡è©¦)", expanded=False):
                                    for b_idx, b_data in sorted(batches.items()):
                                        b_status = b_data.get('status', 'unknown')
                                        b_icon = "âœ…" if b_status == "success" else "âŒ"
                                        col_b1, col_b2 = st.columns([3, 1])
                                        col_b1.write(f"Batch {b_idx+1}: {b_icon}")
                                        if col_b2.button("é‡è©¦", key=f"retry_{f_record['id']}_{b_idx}"):
                                            process_file_in_batches(f_record['filename'], api_key_input, f_record['id'], target_batch_idx=b_idx)
                            st.divider()

# === Tab 3: Review ===
with tab_review:
    st.subheader("åŒ¯å…¥æ ¡å°èˆ‡æˆªåœ–")
    cloud_files = cloud_manager.load_file_records()
    processed_files = [f for f in cloud_files if f.get('ai_status') == 'å·²è¾¨è­˜']
    
    if not processed_files:
        st.warning("æ²’æœ‰å·²è¾¨è­˜å®Œæˆçš„æª”æ¡ˆã€‚è«‹å…ˆè‡³ Tab 2 åŸ·è¡Œ AI è¾¨è­˜ã€‚")
    else:
        file_options = {f['filename']: f['id'] for f in processed_files}
        selected_filename = st.selectbox("é¸æ“‡è¦æ ¡å°çš„æª”æ¡ˆ", list(file_options.keys()))
        selected_file_id = file_options[selected_filename]
        
        all_candidates = []
        batches = cloud_manager.load_temp_batches(selected_file_id)
        for b_idx in sorted(batches.keys()):
            b_data = batches[b_idx]
            if b_data.get('data'):
                items = json.loads(b_data['data'])
                all_candidates.extend(items)
        
        if not all_candidates:
            st.info("æ­¤æª”æ¡ˆæ²’æœ‰è¾¨è­˜å‡ºé¡Œç›®ï¼Œæˆ–æš«å­˜è³‡æ–™å·²æ¸…é™¤ã€‚")
        else:
            # åˆ†é 
            ITEMS_PER_PAGE = 5
            if 'review_page' not in st.session_state: st.session_state['review_page'] = 0
            
            total_items = len(all_candidates)
            max_page = (total_items - 1) // ITEMS_PER_PAGE
            
            c_prev, c_info, c_next = st.columns([1, 2, 1])
            with c_prev:
                if st.button("â¬…ï¸ ä¸Šä¸€é ", disabled=(st.session_state['review_page'] == 0)):
                    st.session_state['review_page'] -= 1
                    st.rerun()
            with c_next:
                if st.button("ä¸‹ä¸€é  â¡ï¸", disabled=(st.session_state['review_page'] >= max_page)):
                    st.session_state['review_page'] += 1
                    st.rerun()
            
            start_idx = st.session_state['review_page'] * ITEMS_PER_PAGE
            end_idx = min(start_idx + ITEMS_PER_PAGE, total_items)
            
            # ä¸‹è¼‰ PDF ç”¨æ–¼æˆªåœ–
            if 'current_pdf_bytes' not in st.session_state or st.session_state.get('current_pdf_name') != selected_filename:
                record = cloud_manager.check_file_exists(selected_filename)
                if record and record.get('blob_name'):
                    st.session_state['current_pdf_bytes'] = cloud_manager.download_blob(record['blob_name'])
                    st.session_state['current_pdf_name'] = selected_filename

            with st.form(key=f"review_form_{selected_file_id}_{st.session_state['review_page']}"):
                for i, item in enumerate(all_candidates[start_idx:end_idx]):
                    real_idx = start_idx + i
                    st.markdown(f"**ç¬¬ {item.get('number', '?')} é¡Œ**")
                    if item.get('type') == "Group": st.info("ğŸ“– é¡Œçµ„")
                    
                    c1, c2 = st.columns([1, 1])
                    with c1:
                        st.text_area("é¡Œç›®", item.get('content', ''), key=f"c_{real_idx}")
                        if item.get('type') != "Group":
                            opts = item.get('options', [])
                            st.text_area("é¸é …", "\n".join(opts) if opts else "", key=f"o_{real_idx}")
                        st.text_input("ç­”æ¡ˆ", item.get('answer', ''), key=f"a_{real_idx}")
                    with c2:
                        st.write("æˆªåœ–å€åŸŸ")
                        # é€™è£¡éœ€é…åˆ smart_importer çš„ ref_imageï¼Œè‹¥è¦å¯¦ä½œéœ€å°‡ image_bytes è½‰ç‚º base64 å­˜å…¥ temp_batches
                        st.info("åœ–ç‰‡æš«å­˜æ–¼è³‡æ–™åº«ï¼Œæ­¤è™•é è¦½éœ€é¡å¤–è™•ç†")

                st.form_submit_button("æš«å­˜ä¿®æ”¹")
            
            st.divider()
            if st.button("âœ… ç¢ºèªåŒ¯å…¥é¡Œåº« (æ¸…é™¤æš«å­˜)", type="primary"):
                progress_bar = st.progress(0)
                count = 0
                for idx, item in enumerate(all_candidates):
                    # è½‰æ›ä¸¦å„²å­˜ (çœç•¥è©³ç´°æ¬„ä½å°æ‡‰)
                    # new_q = Question(...)
                    # cloud_manager.save_question(new_q.to_dict())
                    count += 1
                    progress_bar.progress((idx + 1) / len(all_candidates))
                cloud_manager.clear_temp_batches(selected_file_id)
                st.success(f"æˆåŠŸåŒ¯å…¥ {count} é¡Œï¼")
                st.rerun()

# === Tab 4: Bank ===
with tab_bank:
    st.subheader("é¡Œåº«ç¸½è¦½")
    # ... (ä¿æŒåŸæ¨£)
